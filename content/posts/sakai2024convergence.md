+++
title = "Convergence of Riemannian stochastic gradient descent on Hadamard manifold"
date = "2025-04-07"
tags = ["査読付き原著論文"]
+++

# Convergence of Riemannian stochastic gradient descent on Hadamard manifold
- Author: **Hiroyuki Sakai**, Hideaki Iiduka
- Journal: [Pacific Journal of Optimization](http://www.ybook.co.jp/pjo.html) 20 (4): 743-767 (2024)
- URL: http://yokohamapublishers.jp/online2/oppjo/vol20/p743.html

## Abstract
Riemannian stochastic gradient descent (RSGD) is the most basic Riemannian stochastic optimization algorithm and is used in many applications of machine learning. This study presents novel convergence analyses of RSGD on a Hadamard manifold that incorporate the mini-batch strategy used in deep learning and overcome several problems with the previous analyses. Four types of convergence analysis are described for both constant and diminishing step sizes. The number of steps needed for RSGD convergence is shown to be a convex monotone decreasing function of the batch size. Application of RSGD with several batch sizes to a Riemannian stochastic optimization problem on a symmetric positive-definite manifold theoretically shows that increasing the batch size improves RSGD performance. A numerical evaluation of the relationship between batch size and RSGD performance provides evidence supporting the theoretical results.

## BibTeX
```bibtex
@article{sakai2024convergence,
  title={Convergence of {R}iemannian stochastic gradient descent on {H}adamard manifolds},
  author={Sakai, Hiroyuki and Iiduka, Hideaki},
  journal={Pacific Journal of Optimization},
  volume={20},
  number={4},
  pages={743--767},
  year={2024},
  publisher={Yokohama Publishers}
}
```