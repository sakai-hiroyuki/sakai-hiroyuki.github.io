<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>236236P</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on 236236P</description>
    <generator>Hugo</generator>
    <language>jp</language>
    <lastBuildDate>Wed, 28 Aug 2024 02:23:49 +0900</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds</title>
      <link>http://localhost:1313/posts/sakai2024modified/</link>
      <pubDate>Wed, 28 Aug 2024 02:23:49 +0900</pubDate>
      <guid>http://localhost:1313/posts/sakai2024modified/</guid>
      <description>Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Journal of Optimization Theory and Applications 202: 834–853, 2024. URL: https://link.springer.com/article/10.1007/s10957-024-02449-8?utm_source=rct_congratemailt&amp;utm_medium=email&amp;utm_campaign=oa_20240529&amp;utm_content=10.1007%2Fs10957-024-02449-8 Abstract Link to heading This paper presents modified memoryless quasi-Newton methods based on the spectral-scaling Broyden family on Riemannian manifolds. The method involves adding one parameter to the search direction of the memoryless self-scaling Broyden family on the manifold. Moreover, it uses a general map instead of vector transport.</description>
    </item>
    <item>
      <title>Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method</title>
      <link>http://localhost:1313/posts/sakai2023global/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2023global/</guid>
      <description>Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method Link to heading Author: Hiroyuki Sakai, Hiroyuki Sato, Hideaki Iiduka Journal: Applied Mathematics and Computation 441: 127685 (2023). URL: https://www.sciencedirect.com/science/article/abs/pii/S0096300322007536?via%3Dihub Abstract Link to heading This paper presents the Hager–Zhang (HZ)-type Riemannian conjugate gradient method that uses the exponential retraction. We also present global convergence analyses of our proposed method under two kinds of assumptions. Moreover, we numerically compare our proposed methods with the existing methods by solving two kinds of Riemannian optimization problems on the unit sphere.</description>
    </item>
    <item>
      <title>Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing</title>
      <link>http://localhost:1313/posts/sakai2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2022riemannian/</guid>
      <description>Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: IEEE Transactions on Cybernetics 52 (8): 7328–7339 (2022) URL: https://ieeexplore.ieee.org/document/9339934 Abstract Link to heading This article proposes a Riemannian adaptive optimization algorithm to optimize the parameters of deep neural networks. The algorithm is an extension of both AMSGrad in Euclidean space and RAMSGrad on a Riemannian manifold. The algorithm helps to resolve two issues affecting RAMSGrad.</description>
    </item>
    <item>
      <title>Riemannian Stochastic Fixed Point Optimization Algorithm</title>
      <link>http://localhost:1313/posts/iiduka2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/iiduka2022riemannian/</guid>
      <description>Riemannian Stochastic Fixed Point Optimization Algorithm Link to heading Author: Hideaki Iiduka, Hiroyuki Sakai Journal: Numerical Algorithms 90: 1493–1517 (2022) URL: https://link.springer.com/article/10.1007/s11075-021-01238-y Abstract Link to heading This paper considers a stochastic optimization problem over the fixed point sets of quasinonexpansive mappings on Riemannian manifolds. The problem enables us to consider Riemannian hierarchical optimization problems over complicated sets, such as the intersection of many closed convex sets, the set of all minimizers of a nonsmooth convex function, and the intersection of sublevel sets of nonsmooth convex functions.</description>
    </item>
    <item>
      <title>Hybrid Riemannian conjugate gradient methods with global convergence properties</title>
      <link>http://localhost:1313/posts/sakai2020hybrid/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2020hybrid/</guid>
      <description>Hybrid Riemannian conjugate gradient methods with global convergence properties Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Computational Optimization and Applications 77: 811-830 (2020) URL: https://link.springer.com/article/10.1007/s10589-020-00224-9 GitHub: https://github.com/iiduka-researches/202008-hybrid-rcg Abstract Link to heading This paper presents Riemannian conjugate gradient methods and global convergence analyses under the strong Wolfe conditions. The main idea of the proposed methods is to combine the good global convergence properties of the Dai–Yuan method with the efficient numerical performance of the Hestenes–Stiefel method.</description>
    </item>
    <item>
      <title>Sufficient Descent Riemannian Conjugate Gradient Methods</title>
      <link>http://localhost:1313/posts/sakai2021sufficient/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2021sufficient/</guid>
      <description>Sufficient Descent Riemannian Conjugate Gradient Methods Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Journal of Optimization Theory and Applications 190: 130-150 (2021) URL: https://link.springer.com/article/10.1007/s10957-021-01874-3 GitHub: https://github.com/iiduka-researches/202104-sufficient Abstract Link to heading This paper considers sufficient descent Riemannian conjugate gradient methods with line search algorithms. We propose two kinds of sufficient descent nonlinear conjugate gradient method and prove that these methods satisfy the sufficient descent condition on Riemannian manifolds. One is a hybrid method combining a Fletcher–Reeves-type method with a Polak–Ribière–Polyak-type method, and the other is a Hager–Zhang-type method, both of which are generalizations of those used in Euclidean space.</description>
    </item>
    <item>
      <title>Contact</title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description>E-mail: sakai0815@cs.meiji.ac.jp</description>
    </item>
    <item>
      <title>Publications</title>
      <link>http://localhost:1313/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/</guid>
      <description>査読付き原著論文 Link to heading Hiroyuki Sakai, Hideaki Iiduka: Hybrid Riemannian conjugate gradient methods with global convergence properties, Computational Optimization and Applications 77: 811-830 (2020).&#xA;Hiroyuki Sakai, Hideaki Iiduka: Sufficient Descent Riemannian Conjugate Gradient Methods, Journal of Optimization Theory and Applications 190: 130-150 (2021).&#xA;Hiroyuki Sakai, Hideaki Iiduka: Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing, IEEE Transactions on Cybernetics 52 (8): 7328–7339 (2022).&#xA;Hideaki Iiduka, Hiroyuki Sakai: Riemannian Stochastic Fixed Point Optimization Algorithm, Numerical Algorithms 90: 1493–1517 (2022).</description>
    </item>
  </channel>
</rss>
