<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hiroyuki Sakai</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Hiroyuki Sakai</description>
    <generator>Hugo</generator>
    <language>jp</language>
    <lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A general framework of Riemannian adaptive optimization methods with a convergence analysis</title>
      <link>http://localhost:1313/posts/sakai2025general/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2025general/</guid>
      <description>A general framework of Riemannian adaptive optimization methods with a convergence analysis Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Transactions on Machine Learning Research (2025) URL: https://openreview.net/forum?id=knv4lQFVoE Abstract Link to heading This paper proposes a general framework of Riemannian adaptive optimization methods. The framework encapsulates several stochastic optimization algorithms on Riemannian manifolds and incorporates the mini-batch strategy that is often used in deep learning. Within this framework, we also propose AMSGrad on embedded submanifolds of Euclidean space.</description>
    </item>
    <item>
      <title>Convergence of Riemannian stochastic gradient descent on Hadamard manifold</title>
      <link>http://localhost:1313/posts/sakai2024convergence/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2024convergence/</guid>
      <description>Convergence of Riemannian stochastic gradient descent on Hadamard manifold Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Pacific Journal of Optimization 20 (4): 743-767 (2024) URL: http://yokohamapublishers.jp/online2/oppjo/vol20/p743.html Abstract Link to heading Riemannian stochastic gradient descent (RSGD) is the most basic Riemannian stochastic optimization algorithm and is used in many applications of machine learning. This study presents novel convergence analyses of RSGD on a Hadamard manifold that incorporate the mini-batch strategy used in deep learning and overcome several problems with the previous analyses.</description>
    </item>
    <item>
      <title>Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds</title>
      <link>http://localhost:1313/posts/sakai2024modified/</link>
      <pubDate>Wed, 28 Aug 2024 02:23:49 +0900</pubDate>
      <guid>http://localhost:1313/posts/sakai2024modified/</guid>
      <description>Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Journal of Optimization Theory and Applications 202: 834–853, 2024. URL: https://link.springer.com/article/10.1007/s10957-024-02449-8?utm_source=rct_congratemailt&amp;utm_medium=email&amp;utm_campaign=oa_20240529&amp;utm_content=10.1007%2Fs10957-024-02449-8 Abstract Link to heading This paper presents modified memoryless quasi-Newton methods based on the spectral-scaling Broyden family on Riemannian manifolds. The method involves adding one parameter to the search direction of the memoryless self-scaling Broyden family on the manifold. Moreover, it uses a general map instead of vector transport.</description>
    </item>
    <item>
      <title>Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method</title>
      <link>http://localhost:1313/posts/sakai2023global/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2023global/</guid>
      <description>Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method Link to heading Author: Hiroyuki Sakai, Hiroyuki Sato, Hideaki Iiduka Journal: Applied Mathematics and Computation 441: 127685 (2023). URL: https://www.sciencedirect.com/science/article/abs/pii/S0096300322007536?via%3Dihub Abstract Link to heading This paper presents the Hager–Zhang (HZ)-type Riemannian conjugate gradient method that uses the exponential retraction. We also present global convergence analyses of our proposed method under two kinds of assumptions. Moreover, we numerically compare our proposed methods with the existing methods by solving two kinds of Riemannian optimization problems on the unit sphere.</description>
    </item>
    <item>
      <title>Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing</title>
      <link>http://localhost:1313/posts/sakai2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2022riemannian/</guid>
      <description>Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: IEEE Transactions on Cybernetics 52 (8): 7328–7339 (2022) URL: https://ieeexplore.ieee.org/document/9339934 Abstract Link to heading This article proposes a Riemannian adaptive optimization algorithm to optimize the parameters of deep neural networks. The algorithm is an extension of both AMSGrad in Euclidean space and RAMSGrad on a Riemannian manifold. The algorithm helps to resolve two issues affecting RAMSGrad.</description>
    </item>
    <item>
      <title>Riemannian Stochastic Fixed Point Optimization Algorithm</title>
      <link>http://localhost:1313/posts/iiduka2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/iiduka2022riemannian/</guid>
      <description>Riemannian Stochastic Fixed Point Optimization Algorithm Link to heading Author: Hideaki Iiduka, Hiroyuki Sakai Journal: Numerical Algorithms 90: 1493–1517 (2022) URL: https://link.springer.com/article/10.1007/s11075-021-01238-y Abstract Link to heading This paper considers a stochastic optimization problem over the fixed point sets of quasinonexpansive mappings on Riemannian manifolds. The problem enables us to consider Riemannian hierarchical optimization problems over complicated sets, such as the intersection of many closed convex sets, the set of all minimizers of a nonsmooth convex function, and the intersection of sublevel sets of nonsmooth convex functions.</description>
    </item>
    <item>
      <title>Hybrid Riemannian conjugate gradient methods with global convergence properties</title>
      <link>http://localhost:1313/posts/sakai2020hybrid/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2020hybrid/</guid>
      <description>Hybrid Riemannian conjugate gradient methods with global convergence properties Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Computational Optimization and Applications 77: 811-830 (2020) URL: https://link.springer.com/article/10.1007/s10589-020-00224-9 GitHub: https://github.com/iiduka-researches/202008-hybrid-rcg Abstract Link to heading This paper presents Riemannian conjugate gradient methods and global convergence analyses under the strong Wolfe conditions. The main idea of the proposed methods is to combine the good global convergence properties of the Dai–Yuan method with the efficient numerical performance of the Hestenes–Stiefel method.</description>
    </item>
    <item>
      <title>Sufficient Descent Riemannian Conjugate Gradient Methods</title>
      <link>http://localhost:1313/posts/sakai2021sufficient/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sakai2021sufficient/</guid>
      <description>Sufficient Descent Riemannian Conjugate Gradient Methods Link to heading Author: Hiroyuki Sakai, Hideaki Iiduka Journal: Journal of Optimization Theory and Applications 190: 130-150 (2021) URL: https://link.springer.com/article/10.1007/s10957-021-01874-3 GitHub: https://github.com/iiduka-researches/202104-sufficient Abstract Link to heading This paper considers sufficient descent Riemannian conjugate gradient methods with line search algorithms. We propose two kinds of sufficient descent nonlinear conjugate gradient method and prove that these methods satisfy the sufficient descent condition on Riemannian manifolds. One is a hybrid method combining a Fletcher–Reeves-type method with a Polak–Ribière–Polyak-type method and the other is a Hager–Zhang-type method, both of which are generalizations of those used in Euclidean space.</description>
    </item>
    <item>
      <title>研究業績</title>
      <link>http://localhost:1313/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/publications/</guid>
      <description>査読付き原著論文 Link to heading Hiroyuki Sakai, Hideaki Iiduka: Hybrid Riemannian conjugate gradient methods with global convergence properties, Computational Optimization and Applications 77: 811-830 (2020).&#xA;Hiroyuki Sakai, Hideaki Iiduka: Sufficient Descent Riemannian Conjugate Gradient Methods, Journal of Optimization Theory and Applications 190: 130-150 (2021).&#xA;Hiroyuki Sakai, Hideaki Iiduka: Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing, IEEE Transactions on Cybernetics 52 (8): 7328–7339 (2022).&#xA;Hideaki Iiduka, Hiroyuki Sakai: Riemannian Stochastic Fixed Point Optimization Algorithm, Numerical Algorithms 90: 1493–1517 (2022).</description>
    </item>
    <item>
      <title>経歴</title>
      <link>http://localhost:1313/work/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/work/</guid>
      <description>学歴 Link to heading 2017年4月〜2021年3月：明治大学理工学部情報科学科 2021年4月〜2023年3月：明治大学大学院 理工学研究科 情報科学専攻 博士前期課程 2023年4月〜現在：明治大学大学院 理工学研究科 情報科学専攻 博士後期課程&#xA;職歴 Link to heading 2021年4月〜2023年3月：明治大学理工学部TA 2023年4月〜現在：日本学術振興会特別研究員（DC1）&#xA;学位 Link to heading 修士（理学）取得機関：明治大学、取得年月日 2023年3月26日</description>
    </item>
    <item>
      <title>連絡先</title>
      <link>http://localhost:1313/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/contact/</guid>
      <description>E-mail: sakai0815@cs.meiji.ac.jp</description>
    </item>
  </channel>
</rss>
