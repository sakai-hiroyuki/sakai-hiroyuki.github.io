<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>査読付き原著論文 on Hiroyuki Sakai</title>
    <link>https://sakai-hiroyuki.github.io/tags/%E6%9F%BB%E8%AA%AD%E4%BB%98%E3%81%8D%E5%8E%9F%E8%91%97%E8%AB%96%E6%96%87/</link>
    <description>Recent content in 査読付き原著論文 on Hiroyuki Sakai</description>
    <generator>Hugo</generator>
    <language>jp</language>
    <lastBuildDate>Mon, 07 Apr 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://sakai-hiroyuki.github.io/tags/%E6%9F%BB%E8%AA%AD%E4%BB%98%E3%81%8D%E5%8E%9F%E8%91%97%E8%AB%96%E6%96%87/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A general framework of Riemannian adaptive optimization methods with a convergence analysis</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2025general/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2025general/</guid>
      <description>&lt;h1 id=&#34;a-general-framework-of-riemannian-adaptive-optimization-methods-with-a-convergence-analysis&#34;&gt;&#xA;  A general framework of Riemannian adaptive optimization methods with a convergence analysis&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#a-general-framework-of-riemannian-adaptive-optimization-methods-with-a-convergence-analysis&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://jmlr.org/tmlr/&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Transactions on Machine Learning Research&lt;/a&gt; (2025)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://openreview.net/forum?id=knv4lQFVoE&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://openreview.net/forum?id=knv4lQFVoE&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper proposes a general framework of Riemannian adaptive optimization methods. The framework encapsulates several stochastic optimization algorithms on Riemannian manifolds and incorporates the mini-batch strategy that is often used in deep learning. Within this framework, we also propose AMSGrad on embedded submanifolds of Euclidean space. Moreover, we give convergence analyses valid for both a constant and a diminishing step size. Our analyses also reveal the relationship between the convergence rate and mini-batch size. In numerical experiments, we applied the proposed algorithm to principal component analysis and the low-rank matrix completion problem, which can be considered to be Riemannian optimization problems. Python implementations of the methods used in the numerical experiments are available at &lt;a href=&#34;https://github.com/iiduka-researches/202408-adaptive&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/iiduka-researches/202408-adaptive&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Convergence of Riemannian stochastic gradient descent on Hadamard manifold</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2024convergence/</link>
      <pubDate>Mon, 07 Apr 2025 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2024convergence/</guid>
      <description>&lt;h1 id=&#34;convergence-of-riemannian-stochastic-gradient-descent-on-hadamard-manifold&#34;&gt;&#xA;  Convergence of Riemannian stochastic gradient descent on Hadamard manifold&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#convergence-of-riemannian-stochastic-gradient-descent-on-hadamard-manifold&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;http://www.ybook.co.jp/pjo.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pacific Journal of Optimization&lt;/a&gt; 20 (4): 743-767 (2024)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;http://yokohamapublishers.jp/online2/oppjo/vol20/p743.html&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://yokohamapublishers.jp/online2/oppjo/vol20/p743.html&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;Riemannian stochastic gradient descent (RSGD) is the most basic Riemannian stochastic optimization algorithm and is used in many applications of machine learning. This study presents novel convergence analyses of RSGD on a Hadamard manifold that incorporate the mini-batch strategy used in deep learning and overcome several problems with the previous analyses. Four types of convergence analysis are described for both constant and diminishing step sizes. The number of steps needed for RSGD convergence is shown to be a convex monotone decreasing function of the batch size. Application of RSGD with several batch sizes to a Riemannian stochastic optimization problem on a symmetric positive-definite manifold theoretically shows that increasing the batch size improves RSGD performance. A numerical evaluation of the relationship between batch size and RSGD performance provides evidence supporting the theoretical results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2024modified/</link>
      <pubDate>Wed, 28 Aug 2024 02:23:49 +0900</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2024modified/</guid>
      <description>&lt;h1 id=&#34;modified-memoryless-spectral-scaling-broyden-family-on-riemannian-manifolds&#34;&gt;&#xA;  Modified Memoryless Spectral-Scaling Broyden Family on Riemannian Manifolds&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#modified-memoryless-spectral-scaling-broyden-family-on-riemannian-manifolds&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://link.springer.com/journal/10957&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Journal of Optimization Theory and Applications&lt;/a&gt; 202: 834–853, 2024.&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://link.springer.com/article/10.1007/s10957-024-02449-8?utm_source=rct_congratemailt&amp;amp;utm_medium=email&amp;amp;utm_campaign=oa_20240529&amp;amp;utm_content=10.1007%2Fs10957-024-02449-8&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://link.springer.com/article/10.1007/s10957-024-02449-8?utm_source=rct_congratemailt&amp;utm_medium=email&amp;utm_campaign=oa_20240529&amp;utm_content=10.1007%2Fs10957-024-02449-8&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper presents modified memoryless quasi-Newton methods based on the spectral-scaling Broyden family on Riemannian manifolds. The method involves adding one parameter to the search direction of the memoryless self-scaling Broyden family on the manifold. Moreover, it uses a general map instead of vector transport. This idea has already been proposed within a general framework of Riemannian conjugate gradient methods where one can use vector transport, scaled vector transport, or an inverse retraction. We show that the search direction satisfies the sufficient descent condition under some assumptions on the parameters. In addition, we show global convergence of the proposed method under the Wolfe conditions. We numerically compare it with existing methods, including Riemannian conjugate gradient methods and the memoryless spectral-scaling Broyden family. The numerical results indicate that the proposed method with the BFGS formula is suitable for solving an off-diagonal cost function minimization problem on an oblique manifold.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2023global/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2023global/</guid>
      <description>&lt;h1 id=&#34;global-convergence-of-hager-zhang-type-riemannian-conjugate-gradient-method&#34;&gt;&#xA;  Global Convergence of Hager-Zhang type Riemannian Conjugate Gradient Method&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#global-convergence-of-hager-zhang-type-riemannian-conjugate-gradient-method&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hiroyuki Sato, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://www.sciencedirect.com/journal/applied-mathematics-and-computation&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Applied Mathematics and Computation&lt;/a&gt; 441: 127685 (2023).&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://www.sciencedirect.com/science/article/abs/pii/S0096300322007536?via%3Dihub&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.sciencedirect.com/science/article/abs/pii/S0096300322007536?via%3Dihub&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper presents the Hager–Zhang (HZ)-type Riemannian conjugate gradient method that uses the exponential retraction. We also present global convergence analyses of our proposed method under two kinds of assumptions. Moreover, we numerically compare our proposed methods with the existing methods by solving two kinds of Riemannian optimization problems on the unit sphere. The numerical results show that our proposed method has much better performance than the existing methods, i.e., the FR, DY, PRP, and HS methods. In particular, they show that it has much higher performance than existing methods including the hybrid ones in computing the stability number of graphs problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2022riemannian/</guid>
      <description>&lt;h1 id=&#34;riemannian-adaptive-optimization-algorithm-and-its-application-to-natural-language-processing&#34;&gt;&#xA;  Riemannian Adaptive Optimization Algorithm and Its Application to Natural Language Processing&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#riemannian-adaptive-optimization-algorithm-and-its-application-to-natural-language-processing&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6221036&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IEEE Transactions on Cybernetics&lt;/a&gt; 52 (8): 7328–7339 (2022)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://ieeexplore.ieee.org/document/9339934&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://ieeexplore.ieee.org/document/9339934&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This article proposes a Riemannian adaptive optimization algorithm to optimize the parameters of deep neural networks. The algorithm is an extension of both AMSGrad in Euclidean space and RAMSGrad on a Riemannian manifold. The algorithm helps to resolve two issues affecting RAMSGrad. The first is that it can solve the Riemannian stochastic optimization problem directly, in contrast to RAMSGrad which only achieves a low regret. The other is that it can use constant learning rates, which makes it implementable in practice. Additionally, we apply the proposed algorithm to Poincaré embeddings that embed the transitive closure of the WordNet nouns into the Poincaré ball model of hyperbolic space. Numerical experiments show that regardless of the initial value of the learning rate, our algorithm stably converges to the optimal solution and converges faster than the existing algorithms.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Riemannian Stochastic Fixed Point Optimization Algorithm</title>
      <link>https://sakai-hiroyuki.github.io/posts/iiduka2022riemannian/</link>
      <pubDate>Thu, 25 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/iiduka2022riemannian/</guid>
      <description>&lt;h1 id=&#34;riemannian-stochastic-fixed-point-optimization-algorithm&#34;&gt;&#xA;  Riemannian Stochastic Fixed Point Optimization Algorithm&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#riemannian-stochastic-fixed-point-optimization-algorithm&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: Hideaki Iiduka, &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://link.springer.com/journal/11075&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Numerical Algorithms&lt;/a&gt; 90: 1493–1517 (2022)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://link.springer.com/article/10.1007/s11075-021-01238-y&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://link.springer.com/article/10.1007/s11075-021-01238-y&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper considers a stochastic optimization problem over the fixed point sets of quasinonexpansive mappings on Riemannian manifolds. The problem enables us to consider Riemannian hierarchical optimization problems over complicated sets, such as the intersection of many closed convex sets, the set of all minimizers of a nonsmooth convex function, and the intersection of sublevel sets of nonsmooth convex functions. We focus on adaptive learning rate optimization algorithms, which adapt step-sizes (referred to as learning rates in the machine learning field) to find optimal solutions quickly. We then propose a Riemannian stochastic fixed point optimization algorithm, which combines fixed point approximation methods on Riemannian manifolds with the adaptive learning rate optimization algorithms. We also give convergence analyses of the proposed algorithm for nonsmooth convex and smooth nonconvex optimization. The analysis results indicate that, with small constant step-sizes, the proposed algorithm approximates a solution to the problem. Consideration of the case in which step-size sequences are diminishing demonstrates that the proposed algorithm solves the problem with a guaranteed convergence rate. This paper also provides numerical comparisons that demonstrate the effectiveness of the proposed algorithms with formulas based on the adaptive learning rate optimization algorithms, such as Adam and AMSGrad.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hybrid Riemannian conjugate gradient methods with global convergence properties</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2020hybrid/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2020hybrid/</guid>
      <description>&lt;h1 id=&#34;hybrid-riemannian-conjugate-gradient-methods-with-global-convergence-properties&#34;&gt;&#xA;  Hybrid Riemannian conjugate gradient methods with global convergence properties&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#hybrid-riemannian-conjugate-gradient-methods-with-global-convergence-properties&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://link.springer.com/journal/10589&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Optimization and Applications&lt;/a&gt; 77: 811-830 (2020)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://link.springer.com/article/10.1007/s10589-020-00224-9&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://link.springer.com/article/10.1007/s10589-020-00224-9&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/iiduka-researches/202008-hybrid-rcg&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/iiduka-researches/202008-hybrid-rcg&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper presents Riemannian conjugate gradient methods and global convergence analyses under the strong Wolfe conditions. The main idea of the proposed methods is to combine the good global convergence properties of the Dai–Yuan method with the efficient numerical performance of the Hestenes–Stiefel method. One of the proposed algorithms is a generalization to Riemannian manifolds of the hybrid conjugate gradient method of the Dai and Yuan in Euclidean space. The proposed methods are compared well numerically with the existing methods for solving several Riemannian optimization problems. Python implementations of the methods used in the numerical experiments are available at &lt;a href=&#34;https://github.com/iiduka-researches/202008-hybrid-rcg&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/iiduka-researches/202008-hybrid-rcg&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sufficient Descent Riemannian Conjugate Gradient Methods</title>
      <link>https://sakai-hiroyuki.github.io/posts/sakai2021sufficient/</link>
      <pubDate>Tue, 23 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://sakai-hiroyuki.github.io/posts/sakai2021sufficient/</guid>
      <description>&lt;h1 id=&#34;sufficient-descent-riemannian-conjugate-gradient-methods&#34;&gt;&#xA;  Sufficient Descent Riemannian Conjugate Gradient Methods&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#sufficient-descent-riemannian-conjugate-gradient-methods&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Author: &lt;strong&gt;Hiroyuki Sakai&lt;/strong&gt;, Hideaki Iiduka&lt;/li&gt;&#xA;&lt;li&gt;Journal: &lt;a href=&#34;https://link.springer.com/journal/10957&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Journal of Optimization Theory and Applications&lt;/a&gt; 190: 130-150 (2021)&lt;/li&gt;&#xA;&lt;li&gt;URL: &lt;a href=&#34;https://link.springer.com/article/10.1007/s10957-021-01874-3&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://link.springer.com/article/10.1007/s10957-021-01874-3&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;GitHub: &lt;a href=&#34;https://github.com/iiduka-researches/202104-sufficient&#34;  class=&#34;external-link&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/iiduka-researches/202104-sufficient&lt;/a&gt;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;abstract&#34;&gt;&#xA;  Abstract&#xA;  &lt;a class=&#34;heading-link&#34; href=&#34;#abstract&#34;&gt;&#xA;    &lt;i class=&#34;fa-solid fa-link&#34; aria-hidden=&#34;true&#34; title=&#34;Link to heading&#34;&gt;&lt;/i&gt;&#xA;    &lt;span class=&#34;sr-only&#34;&gt;Link to heading&lt;/span&gt;&#xA;  &lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;p&gt;This paper considers sufficient descent Riemannian conjugate gradient methods with line search algorithms. We propose two kinds of sufficient descent nonlinear conjugate gradient method and prove that these methods satisfy the sufficient descent condition on Riemannian manifolds. One is a hybrid method combining a Fletcher–Reeves-type method with a Polak–Ribière–Polyak-type method and the other is a Hager–Zhang-type method, both of which are generalizations of those used in Euclidean space. Moreover, we prove that the hybrid method has a global convergence property under the strong Wolfe conditions and the Hager–Zhang-type method has the sufficient descent property regardless of whether a line search is used or not. Further, we review two kinds of line search algorithm on Riemannian manifolds and numerically compare our generalized methods by solving several Riemannian optimization problems. The results show that the performance of the proposed hybrid methods greatly depends on the type of line search used. Meanwhile, the Hager–Zhang-type method has the fast convergence property regardless of the type of line search used.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
